In order to share the resources of your node properly, you can set resource limits and requests in Kubernetes. This allows you to reserve enough CPU and memory for your application while still maintaining system health. 

## Taints and Tolerations

* Nodes are **tainted** in order to repel work for example, the Master nodes are tainted to no schedule.
  `kubectl describe <workerNodeName>|grep Taints`
  >Taints:             node-role.kubernetes.io/master:NoSchedule

* Tolerations are applied to pods and allow the pods to schedule onto nodes with matching taints.

So Taints and Tolerations work together to define which pods can be provisioned on each node, i.e. a node with a **taint** `myKey=myValue` does not accept any pod if it does not contain a **Toleration** `myKey=myValue`

## Schedule priorities

* Less requested function schedules the pods in the nodes with less resources usage
* Most requested function schedules the pods in the nodes with more resources usage

This can be configured on the scheduler.

## Limits and requests

You can create a limit and a request when creating pods, both of them **require** the amount of specifications defined on them but:

* **Limits** does not allow the pod to use more resources than the amount specified.
* **Requests** allows the pod to use more resources than the amount specified.

### Hands-on

* View the capacity and the allocatable info from a node

    `kubectl describe nodes`

* The pod YAML for a pod with requests

```
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod1
spec:
  nodeSelector:
    kubernetes.io/hostname: "YOURNODEHOSTNAME"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod1
    resources:
      requests:
        cpu: 800m
        memory: 20Mi
```

* Create the requests pod

    `kubectl create -f resource-pod1.yaml`

* View the pods and nodes they landed on

    `kubectl get pods -o wide`

* The YAML for a pod that has a large request

```
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod2
spec:
  nodeSelector:
    kubernetes.io/hostname: "YOURNODEHOSTNAME"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod2
    resources:
      requests:
        cpu: 1000m
        memory: 20Mi
```

* Create the pod with 1000 millicore request

    `kubectl create -f resource-pod2.yaml`

* See why the pod with a large request didnâ€™t get scheduled

    `kubectl describe pod resource-pod2`

* Look at the total requests per node

    `kubectl describe nodes chadcrowell3c.mylabserver.com`

* Delete the first pod to make room for the pod with a large request

    `kubectl delete pods resource-pod1`

* Watch as the first pod is terminated and the second pod is started

    `kubectl get pods -o wide -w`

* The YAML for a pod that has limits

```
apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1
        memory: 20Mi
```

* Create a pod with limits

    `kubectl create -f limited-pod.yaml`

* Use the exec utility to use the top command

    `kubectl exec -it limited-pod top`